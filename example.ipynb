{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "# Assume that you have 12GB of GPU memory and want to allocate ~4GB: \n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf.gpu_options.allow_growth = True # 탄력적으로 memory 사용\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from keras.layers import Input, Dense,LSTM,RepeatVector,GRU,Dropout,Reshape\n",
    "from keras.layers import*\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "# from deap import base, creator, tools, algorithms\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[:,0][-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = []\n",
    "\tfor x in X:\n",
    "   \t\t new_row = new_row+[i for i in x]\n",
    "            \n",
    "\tnew_row.append(value) \n",
    "\tnew_row_2 = np.array(new_row)\n",
    "\tnew_row_2 = new_row_2.reshape((1,new_row_2.shape[0]))\n",
    "\tinverted = scaler.inverse_transform(new_row_2)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "\n",
    "def create_dataset(dataset,features, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],features))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data,features, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tprint(f\"n_vars = {n_vars}\")\n",
    "\tx = np.zeros(features)\n",
    "\tprint(f\"x = {x}\")\n",
    "\tprint(f\"data.shape is {data.shape}\")\n",
    "\tfor i in range(n_in):\n",
    "\t\tdata = np.insert(data,x,0)\n",
    "\t\tprint(f\"{i}... insert x={x} to data at 0 index\")\n",
    "\t\tprint(f\"\\tdata.shape is {data.shape}\")\n",
    "\t\tprint(f\"\\tdata[{features}*{i}:{features}*{i}+{n_in}] = \"\n",
    "              f\"{data[features*i:features*i+n_in]}\")\n",
    "\tdata = data.reshape(int(data.shape[0]/features),features) \n",
    "\tprint(f\"data reshape to {data.shape}\")\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\tprint(\"Generaget cols and it's names...\")\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(batch_size, X.shape[0], X.shape[1])\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "\n",
    "def read_data(): \n",
    "\n",
    "\t\n",
    "\twindow_size = 0\n",
    "\tfeatures = 8\n",
    "\n",
    "\n",
    "\tseries = read_csv('pollution.csv', header=0, index_col=0)\t\n",
    "\traw_values = series.values\n",
    "\n",
    "\t# integer encode wind direction\n",
    "\tencoder = LabelEncoder()\n",
    "\traw_values[:,4] = encoder.fit_transform(raw_values[:,4])\n",
    "\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\n",
    "\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,features,window_size)\n",
    "\n",
    "\n",
    "\treturn dataset,raw_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,raw_values = read_data()\n",
    "\n",
    "\n",
    "space = {'seq_len':hp.choice('seq_len',[5,10,15,20,25,30]),\n",
    "         'epochs_pre':hp.choice('epochs_pre',[i for i in range(50,1000)]),\n",
    "         'epochs_finetune':hp.choice('epochs_finetune',[i for i in range(50,500)]),\n",
    "         'units':hp.choice('units',[i for i in range(1,50)]),\n",
    "         'dropout':hp.choice('dropout',[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n",
    "         'batch_size':hp.choice('batch_size',[73, 146, 219])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 1, 0.0, ..., 0.8900000000000001, 0, 0],\n",
       "       [11, 4, -1.0, ..., 0.8899999999999997, 0, 0],\n",
       "       [22, 4, 0.0, ..., 1.7900000000000005, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0.0, ..., 4.9199999999999875, 0, 0],\n",
       "       [-2, 0, -1.0, ..., 4.02000000000001, 0, 0],\n",
       "       [4, 1, 1.0, ..., 3.1299999999999955, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43799, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[129, -16, -4.0, ..., 1.79, 0, 0],\n",
       "       [148, -15, -4.0, ..., 2.68, 0, 0],\n",
       "       [159, -11, -5.0, ..., 3.57, 0, 0],\n",
       "       ...,\n",
       "       [10, -22, -3.0, ..., 242.7, 0, 0],\n",
       "       [8, -22, -4.0, ..., 246.72, 0, 0],\n",
       "       [12, -21, -3.0, ..., 249.85, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43800, 8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {'seq_len':hp.choice('seq_len',[5,10,15,20,25,30]),\n",
    "         'epochs_pre':hp.choice('epochs_pre',[i for i in range(50,1000)]),\n",
    "         'epochs_finetune':hp.choice('epochs_finetune',[i for i in range(50,500)]),\n",
    "         'units':hp.choice('units',[i for i in range(1,50)]),\n",
    "         'dropout':hp.choice('dropout',[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]),\n",
    "         'batch_size':hp.choice('batch_size',[73, 146, 219])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'seq_len': 10,\n",
    "          'epochs_pre': 100,\n",
    "          'epochs_finetune': 50,\n",
    "          'units': 20,\n",
    "          'dropout': 0.2,\n",
    "          'batch_size': 73}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params testing:  {'seq_len': 10, 'epochs_pre': 100, 'epochs_finetune': 50, 'units': 20, 'dropout': 0.2, 'batch_size': 73}\n",
      "n_vars = 8\n",
      "x = [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "data.shape is (43799, 8)\n",
      "0... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350400,)\n",
      "\tdata[8*0:8*0+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "1... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350408,)\n",
      "\tdata[8*1:8*1+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "2... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350416,)\n",
      "\tdata[8*2:8*2+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "3... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350424,)\n",
      "\tdata[8*3:8*3+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "4... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350432,)\n",
      "\tdata[8*4:8*4+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "5... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350440,)\n",
      "\tdata[8*5:8*5+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "6... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350448,)\n",
      "\tdata[8*6:8*6+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "7... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350456,)\n",
      "\tdata[8*7:8*7+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "8... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350464,)\n",
      "\tdata[8*8:8*8+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "9... insert x=[0. 0. 0. 0. 0. 0. 0. 0.] to data at 0 index\n",
      "\tdata.shape is (350472,)\n",
      "\tdata[8*9:8*9+10] = [0 0 0 0 0 0 0 0 19 1]\n",
      "data reshape to (43809, 8)\n",
      "Generaget cols and it's names...\n"
     ]
    }
   ],
   "source": [
    "features = 8\n",
    "\n",
    "print ('Params testing: ', params)\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(data,features, params['seq_len'], 1)\n",
    "drop = [i for  i in  range(params['seq_len']*features+1,((params['seq_len']+1)*features))]\n",
    "reframed.drop(reframed.columns[drop], axis=1, inplace=True)\n",
    "reframed = reframed.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30660, 10, 8) (30660,) (4380, 10, 8) (4380,) (8759, 10, 8) (8759,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = 365*24*4\n",
    "train, test = reframed[0:train_size], reframed[train_size:]\n",
    "\n",
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "# divide train into train and valid\n",
    "train2_size = int(365*24*3.5)\n",
    "train_scaled2, valid = train_scaled[0:train2_size], train_scaled[train2_size:]\n",
    "\n",
    "# split into input and outputs\n",
    "x_train,y_train = train_scaled2[:,0:-1],train_scaled2[:,-1]\n",
    "x_valid,y_valid = valid[:,0:-1],valid[:,-1]\n",
    "x_test,y_test = test_scaled[:,0:-1],test_scaled[:,-1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train = x_train.reshape(x_train.shape[0],params['seq_len'],features)\n",
    "x_valid = x_valid.reshape(x_valid.shape[0],params['seq_len'],features)\n",
    "x_test = x_test.reshape(x_test.shape[0],params['seq_len'],features)\n",
    "print(x_train.shape, y_train.shape,x_valid.shape,y_valid.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start pretraining\n",
      "===============\n",
      "Epoch 1/100\n",
      "30660/30660 [==============================] - 2s 73us/step - loss: 0.0435\n",
      "Epoch 2/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0208\n",
      "Epoch 3/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0199\n",
      "Epoch 4/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0194\n",
      "Epoch 5/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0191\n",
      "Epoch 6/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0190\n",
      "Epoch 7/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0188\n",
      "Epoch 8/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0188\n",
      "Epoch 9/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0187\n",
      "Epoch 10/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0186\n",
      "Epoch 11/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0186\n",
      "Epoch 12/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0186\n",
      "Epoch 13/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0186\n",
      "Epoch 14/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0185\n",
      "Epoch 15/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0185\n",
      "Epoch 16/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0185\n",
      "Epoch 17/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0185\n",
      "Epoch 18/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0185\n",
      "Epoch 19/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0185\n",
      "Epoch 20/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0185\n",
      "Epoch 21/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0184\n",
      "Epoch 22/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0184\n",
      "Epoch 23/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0184\n",
      "Epoch 24/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0184\n",
      "Epoch 25/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0183\n",
      "Epoch 26/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0183\n",
      "Epoch 27/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0183\n",
      "Epoch 28/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0183\n",
      "Epoch 29/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0183\n",
      "Epoch 30/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0182\n",
      "Epoch 31/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0182\n",
      "Epoch 32/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0182\n",
      "Epoch 33/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0182\n",
      "Epoch 34/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0182\n",
      "Epoch 35/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0182\n",
      "Epoch 36/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0182\n",
      "Epoch 37/100\n",
      "30660/30660 [==============================] - 2s 60us/step - loss: 0.0181\n",
      "Epoch 38/100\n",
      "30660/30660 [==============================] - 2s 60us/step - loss: 0.0181\n",
      "Epoch 39/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0181\n",
      "Epoch 40/100\n",
      "30660/30660 [==============================] - 2s 60us/step - loss: 0.0181\n",
      "Epoch 41/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0181\n",
      "Epoch 42/100\n",
      "30660/30660 [==============================] - 2s 60us/step - loss: 0.0181\n",
      "Epoch 43/100\n",
      "30660/30660 [==============================] - 2s 60us/step - loss: 0.0181\n",
      "Epoch 44/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0181\n",
      "Epoch 45/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0181\n",
      "Epoch 46/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0181\n",
      "Epoch 47/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0181\n",
      "Epoch 48/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 49/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 50/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0181\n",
      "Epoch 51/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0180\n",
      "Epoch 52/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0180\n",
      "Epoch 53/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 54/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 55/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0181\n",
      "Epoch 56/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 57/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 58/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0180\n",
      "Epoch 59/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 60/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 61/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 62/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0180\n",
      "Epoch 63/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0180\n",
      "Epoch 64/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 65/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 66/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 67/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 68/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0179\n",
      "Epoch 69/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0179\n",
      "Epoch 70/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0179\n",
      "Epoch 71/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0179\n",
      "Epoch 72/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0179\n",
      "Epoch 73/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 74/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 75/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 76/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 77/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 78/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 79/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 80/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0179\n",
      "Epoch 81/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 82/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 83/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0178\n",
      "Epoch 84/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0178\n",
      "Epoch 85/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 86/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0178\n",
      "Epoch 87/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0178\n",
      "Epoch 88/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0178\n",
      "Epoch 89/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 90/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 91/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 92/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 94/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 95/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 96/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 97/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 98/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 99/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n",
      "Epoch 100/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "print('start pretraining')\n",
    "print('===============')\n",
    "\n",
    "# train AE\n",
    "timesteps = x_train.shape[1]\n",
    "input_dim = x_train.shape[2]\n",
    "AE1 = Sequential()\n",
    "AE1.add(CuDNNLSTM(params['units'], batch_input_shape=(params['batch_size'], timesteps, input_dim), stateful=False))\n",
    "AE1.add(RepeatVector(timesteps))\n",
    "AE1.add(CuDNNLSTM(input_dim,stateful = False, return_sequences=True))\n",
    "\n",
    "AE1.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "\n",
    "AE1.fit(x_train, x_train,\n",
    "        epochs=params['epochs_pre'],\n",
    "        batch_size= params['batch_size'],\n",
    "        shuffle=True,\n",
    "        verbose=1)\n",
    "\n",
    "trained_encoder = AE1.layers[0]\n",
    "weights = AE1.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-turning\n",
      "============\n",
      "Epoch 1/50\n",
      "30660/30660 [==============================] - 2s 52us/step - loss: 0.0442\n",
      "Epoch 2/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0036\n",
      "Epoch 3/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0017\n",
      "Epoch 4/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0014\n",
      "Epoch 5/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0012\n",
      "Epoch 6/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0012\n",
      "Epoch 7/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0012\n",
      "Epoch 8/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 9/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 10/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 11/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 12/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 13/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 14/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 15/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 16/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 17/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 18/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 19/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 20/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 21/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 22/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 23/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 24/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 25/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 26/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 27/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 28/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 29/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 30/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 31/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 32/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 33/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 34/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 35/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 36/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 37/50\n",
      "30660/30660 [==============================] - 1s 45us/step - loss: 0.0011\n",
      "Epoch 38/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 39/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 40/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 41/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 42/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 43/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 44/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n",
      "Epoch 45/50\n",
      "30660/30660 [==============================] - 1s 48us/step - loss: 0.0011\n",
      "Epoch 46/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 47/50\n",
      "30660/30660 [==============================] - 1s 48us/step - loss: 0.0011\n",
      "Epoch 48/50\n",
      "30660/30660 [==============================] - 1s 48us/step - loss: 0.0011\n",
      "Epoch 49/50\n",
      "30660/30660 [==============================] - 1s 47us/step - loss: 0.0011\n",
      "Epoch 50/50\n",
      "30660/30660 [==============================] - 1s 46us/step - loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "# Fine-turning\n",
    "print('Fine-turning')\n",
    "print('============')\n",
    "\n",
    "#build finetuning model\n",
    "model1 = Sequential()\n",
    "model1.add(trained_encoder)\n",
    "model1.layers[-1].set_weights(weights)\n",
    "model1.add(Dropout(params['dropout']))\n",
    "model1.add(Dense(1))\n",
    "\n",
    "model1.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "\n",
    "model1.fit(x_train, y_train, epochs=params['epochs_finetune'], batch_size = params['batch_size'], verbose = 1,shuffle=True)\n",
    "\n",
    "model1.save('layer1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start pretraining\n",
      "===============\n",
      "pretrain Autoencoder: 1 ----> Encoder: 25 ----> Epochs: 100\n",
      "(30660, 10, 8)\n",
      "=============================================================\n",
      "Epoch 1/100\n",
      "30660/30660 [==============================] - 2s 78us/step - loss: 0.0351\n",
      "Epoch 2/100\n",
      "30660/30660 [==============================] - 2s 68us/step - loss: 0.0211\n",
      "Epoch 3/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0204\n",
      "Epoch 4/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0196\n",
      "Epoch 5/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0194\n",
      "Epoch 6/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0192\n",
      "Epoch 7/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0191\n",
      "Epoch 8/100\n",
      "30660/30660 [==============================] - 2s 68us/step - loss: 0.0190\n",
      "Epoch 9/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0188\n",
      "Epoch 10/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0187\n",
      "Epoch 11/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0187\n",
      "Epoch 12/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0186\n",
      "Epoch 13/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0186\n",
      "Epoch 14/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0185\n",
      "Epoch 15/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0185\n",
      "Epoch 16/100\n",
      "30660/30660 [==============================] - 2s 61us/step - loss: 0.0185\n",
      "Epoch 17/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0184\n",
      "Epoch 18/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0184\n",
      "Epoch 19/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0184\n",
      "Epoch 20/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0184\n",
      "Epoch 21/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0183\n",
      "Epoch 22/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0183\n",
      "Epoch 23/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0183\n",
      "Epoch 24/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0183\n",
      "Epoch 25/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0183\n",
      "Epoch 26/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0182\n",
      "Epoch 27/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0182\n",
      "Epoch 28/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0182\n",
      "Epoch 29/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0182\n",
      "Epoch 30/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0182\n",
      "Epoch 31/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0182\n",
      "Epoch 32/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0182\n",
      "Epoch 33/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 34/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0181\n",
      "Epoch 35/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 36/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 37/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 38/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0181\n",
      "Epoch 39/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 40/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0180\n",
      "Epoch 41/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0180\n",
      "Epoch 42/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0180\n",
      "Epoch 43/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 44/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 45/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 46/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0180\n",
      "Epoch 47/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 48/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 49/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0180\n",
      "Epoch 50/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 51/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 52/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 53/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 54/100\n",
      "30660/30660 [==============================] - 2s 62us/step - loss: 0.0179\n",
      "Epoch 55/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 56/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 57/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 58/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 59/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 60/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 61/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 62/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 63/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 64/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 65/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 66/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 67/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 68/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0179\n",
      "Epoch 69/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 70/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 71/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 72/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0179\n",
      "Epoch 73/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 74/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 75/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 76/100\n",
      "30660/30660 [==============================] - 2s 63us/step - loss: 0.0178\n",
      "Epoch 77/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0179\n",
      "Epoch 78/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 79/100\n",
      "30660/30660 [==============================] - 2s 66us/step - loss: 0.0178\n",
      "Epoch 80/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 81/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 82/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 83/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 84/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 85/100\n",
      "30660/30660 [==============================] - 2s 64us/step - loss: 0.0178\n",
      "Epoch 86/100\n",
      "30660/30660 [==============================] - 2s 66us/step - loss: 0.0178\n",
      "Epoch 87/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 88/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0178\n",
      "Epoch 89/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 90/100\n",
      "30660/30660 [==============================] - 2s 67us/step - loss: 0.0178\n",
      "Epoch 91/100\n",
      "30660/30660 [==============================] - 2s 68us/step - loss: 0.0178\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 93/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 94/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 95/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 96/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 97/100\n",
      "30660/30660 [==============================] - 2s 66us/step - loss: 0.0178\n",
      "Epoch 98/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0178\n",
      "Epoch 99/100\n",
      "30660/30660 [==============================] - 2s 66us/step - loss: 0.0178\n",
      "Epoch 100/100\n",
      "30660/30660 [==============================] - 2s 65us/step - loss: 0.0177\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4ca543305b8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# update training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mx_train_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# reshape encoded input ot 3D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "print('start pretraining')\n",
    "print('===============')\n",
    "\n",
    "# train AE\n",
    "timesteps = x_train.shape[1]\n",
    "input_dim = x_train.shape[2]\n",
    "hidden_layers = [25, 26]\n",
    "epochs_pre = [100, 100]\n",
    "trained_encoder = []\n",
    "x_train_temp = x_train\n",
    "for i, (hidden, epochs) in enumerate(zip(hidden_layers, epochs_pre), start=1):\n",
    "    print(f\"pretrain Autoencoder: {i} ----> Encoder: {hidden} ----> Epochs: {epochs}\")\n",
    "    print(x_train_temp.shape)\n",
    "    print('=============================================================')\n",
    "    \n",
    "    inputs  = Input(batch_shape=(params['batch_size'], timesteps, x_train_temp.shape[2]))\n",
    "    encoded = CuDNNLSTM(hidden,\n",
    "                        batch_input_shape=(params['batch_size'], timesteps, x_train_temp.shape[2]),\n",
    "                        stateful = False)(inputs)\n",
    "    decoded = RepeatVector(timesteps)(encoded)\n",
    "    decoded = CuDNNLSTM(input_dim,\n",
    "                        stateful=False,\n",
    "                        return_sequences=True)(decoded)\n",
    "    \n",
    "    AE2 = Model(inputs, decoded)\n",
    "    AE2.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "    \n",
    "    AE2.fit(x_train_temp, x_train,\n",
    "            epochs=epochs,\n",
    "            batch_size= params['batch_size'],\n",
    "            shuffle=True,\n",
    "            verbose=1)\n",
    "    \n",
    "    # store trained encoder and its weights\n",
    "    trained_encoder.append((AE2.layers[1], AE2.layers[1].get_weights()))\n",
    "    \n",
    "    # update training data\n",
    "    x_train_temp = encoder.predict(x_train_temp, batch_size=params['batch_size'])\n",
    "    \n",
    "    # reshape encoded input ot 3D\n",
    "    inputs = Input(shape=(x_train_temp.shape[1],))\n",
    "    reshape = RepeatVector(timesteps)(inputs)\n",
    "    Repeat = Model(inputs, reshape)\n",
    "    \n",
    "    x_train_temp = Repeat.predict(x_train_temp, batch_size=params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "epochs_finetune = 463\n",
    "print('\\nFine-tuning')\n",
    "print('============')\n",
    "\n",
    "l = len(trained_encoder)\n",
    "# build finetuning model\n",
    "model = Sequential()\n",
    "for i, encod in enumerate(trained_encoder):\n",
    "    model.add(encod[0])\n",
    "    model.layers[-1].set_weights(encod[1])\n",
    "    model.add(Dropout(dropout))\n",
    "    if (i+1 != 1):\n",
    "        model.add(RepeatVector(timesteps))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam')\n",
    "model.fit(x_train, y_train, epochs=epochs_finetune, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 100/len(A) * np.sum(np.abs(F - A) / (np.abs(A) + np.abs(F))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
